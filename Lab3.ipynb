{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the value of x is 3'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 3\n",
    "f'the value of x is {x}'\n",
    "r''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d', 'd', 'd', 'd', 'd']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sent = \"m.tom.ridddddle.k@hogwarts.com\"\n",
    "re.findall('(d){1}', sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-1.2 12 3.4 -3.6 8.2e10 720p 12']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sent = \"-1.2 12 3.4 -3.6 8.2e10 720p 12\"\n",
    "re.findall('-?\\d+(?:[.e,]\\d+e?\\d*)?(?:\\s|$)', sent)\n",
    "B = re.findall(r\"-?\\d*.*\\d[^(a-z)]$\",sent)\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello there', 'this is ', 'multiline sentence']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import BlanklineTokenizer, WhitespaceTokenizer, LineTokenizer\n",
    "sent = \"\"\"Hello there\n",
    "this is \n",
    "multiline sentence\n",
    "\"\"\"\n",
    "LineTokenizer().tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "doc = \" It is a pleasant evening. U.S.A jo, who came from US arrived at the venue. Food was tasty.\" # try Mr. jo\n",
    "sents = sent_tokenize(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' It is a pleasant evening.',\n",
       " 'U.S.A jo, who came from US arrived at the venue.',\n",
       " 'Food was tasty.']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "import re\n",
    "punkt_param = PunktParameters()\n",
    "punkt_param.abbrev_types = set(['dr', 'vs', 'mr', 'mrs', 'prof', 'inc', 'i.e'])\n",
    "sentence_splitter = PunktSentenceTokenizer(punkt_param)\n",
    "text = \"is THAT what you mean, Mr.. are you sure? yes, i am\"\n",
    "text = re.sub(r'([?!.])', r'\\1 ', text)\n",
    "sentences = sentence_splitter.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is THAT what you mean, Mr. .', 'are you sure?', 'yes, i am']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'hesitate', 'to', 'ask', 'questions', 'U.K', '?']\n",
      "['Don', \"'\", 't', 'hesitate', 'to', 'ask', 'questions', 'U', '.', 'K', '?']\n",
      "[(0, 3), (3, 4), (4, 5), (6, 14), (15, 17), (18, 21), (22, 31), (32, 33), (33, 34), (34, 35), (35, 36)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer, TreebankWordTokenizer\n",
    "sent = \"Don't hesitate to ask questions U.K?\"\n",
    "t2 = TreebankWordTokenizer().tokenize(sent)\n",
    "print(t2)\n",
    "t1 = WordPunctTokenizer().tokenize(sent)\n",
    "print(t1)\n",
    "print(list(WordPunctTokenizer().span_tokenize(sent))) # from nltk.tokenize.util import spans_to_relative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'this', 'sentence', 'h', 've', 'lot', 's', 'of', 'puncts', 'U', 'K']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "sent = \"hello! this, sentence h've lot's of puncts U.K!!?\"\n",
    "toke = RegexpTokenizer('\\w+')\n",
    "toke.tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '!', 'this', ',', 'sentence', 'h', \"'\", 've', 'lot', \"'\", 's', 'of', 'puncts', '!!?']\n"
     ]
    }
   ],
   "source": [
    "### task implement WordPunctTokenizer using RegexpTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "sent = \"hello! this, sentence h've lot's of puncts!!?\"\n",
    "print(WordPunctTokenizer().tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '!', 'this', ',', ',', '.', 'sentence', 'h', \"'\", 've', 'lot', \"'\", 's', 'of', 'puncts', '!', '!', '?']\n"
     ]
    }
   ],
   "source": [
    "sent = \"hello! this,,. sentence h've lot's of puncts!!?\"\n",
    "print(regexp_tokenize(sent, pattern='\\w+|\\S')) # precedence applies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello!', 'this,', 'sentence', \"h've\", \"lot's\", 'of', 'puncts!!?']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=RegexpTokenizer(\"\\S+\")\n",
    "tokenizer.tokenize(\"hello! this, sentence h've lot's of puncts!!?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello!', 'this,', 'sentence', \"h've\", \"lot's\", 'of', 'puncts!!?']"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=RegexpTokenizer('\\s+',gaps=True) # gaps defines the delimeter \n",
    "tokenizer.tokenize(\"hello! this, sentence h've lot's of puncts!!?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### solution bonus separate !!?\n",
    "RegexpTokenizer('\\w+|[^\\w\\s]+').tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "# remove punctuations\n",
    "# convert to lower using sent.lower()\n",
    "# stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I do not like balls are not'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### task replace Don't with Do not\n",
    "sent = \"I don't like balls aren't\"\n",
    "re.sub(r'(\\w+)n\\'t',r'\\1 not', sent) # or \\g<1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.escape(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hello', 'UK', 'sentence', 'h', 'lot', 'puncts']]\n"
     ]
    }
   ],
   "source": [
    "# remove punctuations\n",
    "import re\n",
    "import string\n",
    "text=[\" hello! U.K, sentence h've lot's of puncts!!?.\"]\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_docs=[word_tokenize(doc) for doc in text]\n",
    "x=re.compile(f'[{re.escape(string.punctuation)}]')\n",
    "tokenized_docs_no_punctuation = []\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = x.sub('', token)\n",
    "        if (not new_token == '') and (new_token not in stops):\n",
    "            new_review.append(new_token)\n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "print(tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_stops = {'not', \"aren't\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It', 'pleasant', 'evening', '.']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stop words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stops=set(stopwords.words('english')) - not_stops\n",
    "sent = \" It is a pleasant evening.\"\n",
    "words= word_tokenize(sent)\n",
    "[word for word in words if word not in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuations\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stops=set(stopwords.words('english'))\n",
    "text=[\" It is a pleasant evening.\",\"Guests, who came from US arrived at the venue\",\"Food was tasty.\"]\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_docs=[word_tokenize(doc) for doc in text]\n",
    "x=re.compile(f'[{re.escape(string.punctuation)}]')\n",
    "tokenized_docs_no_punctuation = []\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = x.sub('', token)\n",
    "        if (not new_token == '') and (new_token not in stops):\n",
    "            new_review.append(new_token)\n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "print(tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEWARE OF THE STOP TRAP\n",
    "stops\n",
    "### sentiment performance jumped from 73 to 94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting tokens\n",
    "# replace Don't with Do not {perform before tokenization}\n",
    "# dealing with repeating characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replacers import RepeatReplacer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacer = RepeatReplacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Happy'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacer.replace('Happyy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# https://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.nltk.org/book/ch02.html\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.huihoo.com/nltk/0.9.5/api/nltk.corpus.reader.plaintext.PlaintextCorpusReader-class.html\n",
    "emma = nltk.corpus.gutenberg.words('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.nltk.org/api/nltk.html#nltk.text.Text\n",
    "emma = nltk.Text(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma.vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "emmas = nltk.corpus.gutenberg.raw('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = emmas[:1927]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "letters_only_corpus = re.sub('[^a-zA-Z.]', ' ', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters_only_corpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
